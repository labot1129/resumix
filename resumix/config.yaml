llm:
  model: "deepseek-chat"
  url: "https://api.deepseek.com/v1/chat/completions"
docker_llm:
  model: "gemma3:4b"
  url: "http://host.docker.internal:11434/api/generate"

ocr:
  use_easyocr: True
  use_paddle: False
  easyocr:
    directory: "resumix/models/easyocr"
    gpu: False
